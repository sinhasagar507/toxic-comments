{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"},{"sourceId":6986321,"sourceType":"datasetVersion","datasetId":4015103}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n<a name = Section1></a>\n# **1. Introduction**\n---\n\n- Being anonymous over the internet can sometimes make people say nasty things that they normally would not in real life. \n\n- Often, online platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments. \n\n- To combat this issue, the <a href=\"https://conversationai.github.io/\">Conversation AI team</a>, a research initiative founded by <a href=\"https://jigsaw.google.com/\">Jigsaw</a> and Google (both a part of <a href=\"https://abc.xyz/\">Alphabet</a>) are working on tools to help improve online conversation. One area of focus is the study of **negative online behaviors**, like **toxic comments** (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). \n\n<center><img width=\"50%\" src=\"https://miro.medium.com/v2/resize:fit:679/1*r5OBabkQnYD1D4yzC_kvLQ.gif\"></center>\n\n- So far they have built a range of publicly available models served through the <a href = \"https://perspectiveapi.com/\">Perspective API</a>, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content) ","metadata":{"execution":{"iopub.status.busy":"2023-11-13T05:39:34.707855Z","iopub.execute_input":"2023-11-13T05:39:34.708853Z","iopub.status.idle":"2023-11-13T05:39:34.720784Z","shell.execute_reply.started":"2023-11-13T05:39:34.708808Z","shell.execute_reply":"2023-11-13T05:39:34.719530Z"}}},{"cell_type":"markdown","source":"---\n<a name = Section2></a>\n# **2. Problem Statement**\n---\n\n- In this competition, the task is to build a **multi-headed classification model** that's capable of detecting different types of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective's   <a href=\"https://github.com/conversationai/unintended-ml-bias-analysis\">current models</a>. \n  \n- Let's say you have been assigned the particular task...How would you proceed about it? ","metadata":{}},{"cell_type":"markdown","source":"---\n<a name = Section3></a>\n# **3. Installing & Importing Libraries**\n---","metadata":{}},{"cell_type":"markdown","source":"<a name = Section31></a>\n### **3.1 Installing Libraries**","metadata":{}},{"cell_type":"code","source":"!pip install ydata-profiling                                  # Library to generate basic statistics about data","metadata":{"execution":{"iopub.status.busy":"2023-12-06T09:21:58.361666Z","iopub.execute_input":"2023-12-06T09:21:58.362548Z","iopub.status.idle":"2023-12-06T09:22:31.613030Z","shell.execute_reply.started":"2023-12-06T09:21:58.362507Z","shell.execute_reply":"2023-12-06T09:22:31.611546Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: ydata-profiling in /opt/conda/lib/python3.10/site-packages (4.3.1)\nCollecting scipy<1.11,>=1.4.1 (from ydata-profiling)\n  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas!=1.4.0,<2.1,>1.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (1.5.3)\nRequirement already satisfied: matplotlib<4,>=3.2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (3.7.1)\nRequirement already satisfied: pydantic<2,>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (1.10.9)\nRequirement already satisfied: PyYAML<6.1,>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (6.0)\nRequirement already satisfied: jinja2<3.2,>=2.11.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (3.1.2)\nRequirement already satisfied: visions[type_image_path]==0.7.5 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (0.7.5)\nRequirement already satisfied: numpy<1.24,>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (1.23.5)\nRequirement already satisfied: htmlmin==0.1.12 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (0.1.12)\nRequirement already satisfied: phik<0.13,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (0.12.3)\nRequirement already satisfied: requests<3,>=2.24.0 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (2.31.0)\nRequirement already satisfied: tqdm<5,>=4.48.2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (4.65.0)\nRequirement already satisfied: seaborn<0.13,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (0.12.2)\nRequirement already satisfied: multimethod<2,>=1.4 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (1.9.1)\nRequirement already satisfied: statsmodels<1,>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (0.14.0)\nRequirement already satisfied: typeguard<3,>=2.13.2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (2.13.3)\nRequirement already satisfied: imagehash==4.3.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (4.3.1)\nRequirement already satisfied: wordcloud>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (1.9.2)\nRequirement already satisfied: dacite>=1.8 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (1.8.1)\nRequirement already satisfied: PyWavelets in /opt/conda/lib/python3.10/site-packages (from imagehash==4.3.1->ydata-profiling) (1.4.1)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from imagehash==4.3.1->ydata-profiling) (9.5.0)\nRequirement already satisfied: attrs>=19.3.0 in /opt/conda/lib/python3.10/site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (23.1.0)\nRequirement already satisfied: networkx>=2.4 in /opt/conda/lib/python3.10/site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (3.1)\nRequirement already satisfied: tangled-up-in-unicode>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (0.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2<3.2,>=2.11.1->ydata-profiling) (2.1.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4,>=3.2->ydata-profiling) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4,>=3.2->ydata-profiling) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4,>=3.2->ydata-profiling) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4,>=3.2->ydata-profiling) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4,>=3.2->ydata-profiling) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4,>=3.2->ydata-profiling) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4,>=3.2->ydata-profiling) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas!=1.4.0,<2.1,>1.1->ydata-profiling) (2023.3)\nRequirement already satisfied: joblib>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from phik<0.13,>=0.11.1->ydata-profiling) (1.2.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1.8.1->ydata-profiling) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.24.0->ydata-profiling) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.24.0->ydata-profiling) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.24.0->ydata-profiling) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.24.0->ydata-profiling) (2023.5.7)\nRequirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels<1,>=0.13.2->ydata-profiling) (0.5.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels<1,>=0.13.2->ydata-profiling) (1.16.0)\nInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.11.1\n    Uninstalling scipy-1.11.1:\n      Successfully uninstalled scipy-1.11.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scipy-1.10.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install nltk                                       # Natural Language Toolkit \n!python -m spacy download en_core_web_md                # Spacy NLP ","metadata":{"execution":{"iopub.status.busy":"2023-12-06T09:22:31.615942Z","iopub.execute_input":"2023-12-06T09:22:31.616493Z","iopub.status.idle":"2023-12-06T09:23:21.891076Z","shell.execute_reply.started":"2023-12-06T09:22:31.616447Z","shell.execute_reply":"2023-12-06T09:23:21.889443Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\nCollecting en-core-web-md==3.6.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-md==3.6.0) (3.6.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.10)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.6)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.8)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.3.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.65.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.23.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.10.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (59.8.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.6.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.5.7)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.9)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.0)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.3)\nInstalling collected packages: en-core-web-md\nSuccessfully installed en-core-web-md-3.6.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_md')\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torch-summary                              # Pytorch summary ","metadata":{"execution":{"iopub.status.busy":"2023-12-03T07:59:07.337323Z","iopub.execute_input":"2023-12-03T07:59:07.337620Z","iopub.status.idle":"2023-12-03T07:59:16.900650Z","shell.execute_reply.started":"2023-12-03T07:59:07.337593Z","shell.execute_reply":"2023-12-03T07:59:16.898911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install contractions>0.0.18       # Resolve contractions, for instance, you're -> you are","metadata":{"execution":{"iopub.status.busy":"2023-12-02T15:56:03.962734Z","iopub.execute_input":"2023-12-02T15:56:03.963171Z","iopub.status.idle":"2023-12-02T15:56:18.778633Z","shell.execute_reply.started":"2023-12-02T15:56:03.963127Z","shell.execute_reply":"2023-12-02T15:56:18.776915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vader Sentiment Analyzer \n!pip install vaderSentiment           # Analyze sentiment ","metadata":{"execution":{"iopub.status.busy":"2023-12-02T16:00:33.634671Z","iopub.execute_input":"2023-12-02T16:00:33.635214Z","iopub.status.idle":"2023-12-02T16:00:47.590879Z","shell.execute_reply.started":"2023-12-02T16:00:33.635171Z","shell.execute_reply":"2023-12-02T16:00:47.589537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Language detection in Python \n!pip install langdetect                # Language detection in Python ","metadata":{"execution":{"iopub.status.busy":"2023-12-02T16:00:51.613225Z","iopub.execute_input":"2023-12-02T16:00:51.613684Z","iopub.status.idle":"2023-12-02T16:01:09.269924Z","shell.execute_reply.started":"2023-12-02T16:00:51.613642Z","shell.execute_reply":"2023-12-02T16:01:09.268634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fasttext language detection\n!pip install fasttext-langdetect","metadata":{"execution":{"iopub.status.busy":"2023-11-27T06:36:49.107831Z","iopub.execute_input":"2023-11-27T06:36:49.108270Z","iopub.status.idle":"2023-11-27T06:37:01.593573Z","shell.execute_reply.started":"2023-11-27T06:36:49.108236Z","shell.execute_reply":"2023-11-27T06:37:01.591474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Upgrade all libraries at once \n!pip install --upgrade --upgrade-strategy eager pip","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:03:23.607004Z","iopub.execute_input":"2023-11-27T05:03:23.607837Z","iopub.status.idle":"2023-11-27T05:03:38.656920Z","shell.execute_reply.started":"2023-11-27T05:03:23.607765Z","shell.execute_reply":"2023-11-27T05:03:38.654992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wordcloud","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:05:23.790246Z","iopub.execute_input":"2023-11-27T05:05:23.790858Z","iopub.status.idle":"2023-11-27T05:05:38.289241Z","shell.execute_reply.started":"2023-11-27T05:05:23.790813Z","shell.execute_reply":"2023-11-27T05:05:38.287070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## System Version Check \nimport sys \nprint(f\"Latest Python Version on Kaggle: {sys.version}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-27T06:35:25.861351Z","iopub.execute_input":"2023-11-27T06:35:25.861917Z","iopub.status.idle":"2023-11-27T06:35:25.870245Z","shell.execute_reply.started":"2023-11-27T06:35:25.861872Z","shell.execute_reply":"2023-11-27T06:35:25.868052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name = Section33></a>\n### **3.3 Importing Libraries**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:18:57.930547Z","iopub.execute_input":"2023-12-03T10:18:57.931052Z","iopub.status.idle":"2023-12-03T10:18:57.975236Z","shell.execute_reply.started":"2023-12-03T10:18:57.931012Z","shell.execute_reply":"2023-12-03T10:18:57.974014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport re \nfrom collections import Counter \nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n# Object serialization \nimport pickle\nimport sklearn\n\n# WordCloud \nfrom wordcloud import WordCloud, STOPWORDS\n\n# Data Visualization \nimport matplotlib as mp\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\n\n# Pandas pre-profiling \nfrom ydata_profiling import ProfileReport \n\n# Import Natural Language Processing (NLP) libraries\nimport nltk\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\nnltk.download(\"punkt\")\nnltk.download(\"averaged_perceptron_tagger\")\nfrom nltk import word_tokenize, sent_tokenize \nfrom nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords \n\n# Import Spacy for advanced natural language processing\nimport spacy\n\n# Fasttext languae detection \n# from ftlangdetect import detect\n\n# Contractions \n# import contractions as cm \n\n# Import langdetect for language detection\n# Note: set seed=0 to enforce consistent results (to be done later)\n# from langdetect import DetectorFactory, detect \n\n# Import scikit-learn utilities\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder \nfrom sklearn.pipeline import Pipeline, FeatureUnion  \n\n# Import Spacy tokenizer\nfrom spacy.tokenizer import Tokenizer \n\n# Import transformers for handling pretrained models\nimport transformers \n\n# Import PyTorch for deep learning\nimport torch \nimport torch.nn as nn \nfrom torch.utils.data import Dataset \nfrom torch.utils.data import DataLoader \nimport torch.optim as optim \nimport torch.nn.functional as F \n\n# Import torchsummary for model summary\n# from torchsummary import summary \n\n# Import tqdm for progress bars\nfrom tqdm import tqdm\n\n# Chi2 test \nfrom scipy.stats import chi2_contingency","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:15:35.273808Z","iopub.execute_input":"2023-12-03T11:15:35.274789Z","iopub.status.idle":"2023-12-03T11:15:35.295729Z","shell.execute_reply.started":"2023-12-03T11:15:35.274740Z","shell.execute_reply":"2023-12-03T11:15:35.294465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the constants \nporter_stem = PorterStemmer()\nwordnet_lemma = WordNetLemmatizer()\nstopwords = set(stopwords.words(\"english\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:15:37.675157Z","iopub.execute_input":"2023-12-03T11:15:37.675615Z","iopub.status.idle":"2023-12-03T11:15:37.682659Z","shell.execute_reply.started":"2023-12-03T11:15:37.675577Z","shell.execute_reply":"2023-12-03T11:15:37.681384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m nltk.downloader wordnet\n!unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-11-27T06:43:34.513735Z","iopub.execute_input":"2023-11-27T06:43:34.514293Z","iopub.status.idle":"2023-11-27T06:43:37.824793Z","shell.execute_reply.started":"2023-11-27T06:43:34.514257Z","shell.execute_reply":"2023-11-27T06:43:37.823333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check versions of all tertiary packages \nprint(f\"Sklearn: {sklearn.__version__}\")\nprint(f\"Matplotlib: {mp.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:46:58.846128Z","iopub.execute_input":"2023-12-03T10:46:58.846463Z","iopub.status.idle":"2023-12-03T10:46:58.857655Z","shell.execute_reply.started":"2023-12-03T10:46:58.846434Z","shell.execute_reply":"2023-12-03T10:46:58.856383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.stem import WordNetLemmatizer \nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:46:58.860189Z","iopub.execute_input":"2023-12-03T10:46:58.860563Z","iopub.status.idle":"2023-12-03T10:46:58.875646Z","shell.execute_reply.started":"2023-12-03T10:46:58.860532Z","shell.execute_reply":"2023-12-03T10:46:58.874352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Lemmatizer = WordNetLemmatizer()\nprint(\"words :\", Lemmatizer.lemmatize(\"words\")) \nprint(\"corpora :\", Lemmatizer.lemmatize(\"corpra\")) \n  \n# a denotes adjective in \"pos\" \nprint(\"better :\", Lemmatizer.lemmatize(\"better\", pos =\"a\")) ","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:15:44.390244Z","iopub.execute_input":"2023-12-03T11:15:44.391295Z","iopub.status.idle":"2023-12-03T11:15:44.583169Z","shell.execute_reply.started":"2023-12-03T11:15:44.391243Z","shell.execute_reply":"2023-12-03T11:15:44.581217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textblob import TextBlob","metadata":{"execution":{"iopub.status.busy":"2023-11-27T07:08:14.035870Z","iopub.execute_input":"2023-11-27T07:08:14.036389Z","iopub.status.idle":"2023-11-27T07:08:14.043137Z","shell.execute_reply.started":"2023-11-27T07:08:14.036349Z","shell.execute_reply":"2023-11-27T07:08:14.041841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wiki = TextBlob(\"Python is a high-level, general-purpose programming language.\")\nwiki.tags","metadata":{"execution":{"iopub.status.busy":"2023-11-27T07:08:50.460576Z","iopub.execute_input":"2023-11-27T07:08:50.461264Z","iopub.status.idle":"2023-11-27T07:08:50.473362Z","shell.execute_reply.started":"2023-11-27T07:08:50.461037Z","shell.execute_reply":"2023-11-27T07:08:50.471206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m textblob.download_corpora","metadata":{"execution":{"iopub.status.busy":"2023-11-27T07:11:36.187401Z","iopub.execute_input":"2023-11-27T07:11:36.187987Z","iopub.status.idle":"2023-11-27T07:11:38.733107Z","shell.execute_reply.started":"2023-11-27T07:11:36.187941Z","shell.execute_reply":"2023-11-27T07:11:38.731180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textblob import Word \nw = Word(\"Hello there!!!\")\nw.lemmatize()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T07:11:44.792526Z","iopub.execute_input":"2023-11-27T07:11:44.793005Z","iopub.status.idle":"2023-11-27T07:11:44.983515Z","shell.execute_reply.started":"2023-11-27T07:11:44.792965Z","shell.execute_reply":"2023-11-27T07:11:44.981672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download en_core_web_md","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:20:13.850076Z","iopub.execute_input":"2023-12-03T11:20:13.850616Z","iopub.status.idle":"2023-12-03T11:20:40.008851Z","shell.execute_reply.started":"2023-12-03T11:20:13.850569Z","shell.execute_reply":"2023-12-03T11:20:40.007641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_md\")\n\ndoc = nlp(u\"The shimmering azure waters lapped gently against the golden sands, painting a serene picture of tranquility under the midday sun. A gentle breeze carried the faint scent of salt and sea spray, mingling with the crisp, clean air. Seabirds soared overhead, their graceful arcs cutting through the vast expanse of the sky. Along the coastline, a cluster of cottages nestled among verdant trees, their vibrant colors standing out against the backdrop of lush foliage. Laughter and joyous chatter filled the air as families enjoyed their day by the shore, building sandcastles and playing in the surf. Farther out, sailboats dotted the horizon, their billowing sails catching the ocean's whispers, inviting adventurers to explore the endless mysteries hidden beyond the horizon.\")\n\nfor token in doc:\n    print(token.lemma_)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T07:19:13.679916Z","iopub.execute_input":"2023-11-27T07:19:13.680650Z","iopub.status.idle":"2023-11-27T07:19:16.038008Z","shell.execute_reply.started":"2023-11-27T07:19:13.680596Z","shell.execute_reply":"2023-11-27T07:19:16.036115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n<a name = Section4></a>\n# **4. Data Acquisition & Description**\n---\n\n\n- The dataset consists of comments and its classification into six categories of toxic, severely toxic, obscene, threat, insult and identity hate\n\n| Records(train+test)| Features |  Size(total) |\n| :--: | :--: | :--: |\n| 312735 | 2 | 49.78 MB | \n\n<br>\n\n| # | Feature Name | Feature Description |\n|:--:|:--|:--| \n|1| Id | A unique identifier for a particular comment |\n|2| comment_text | Contains comments taken from Wikipedia talk page edit discussions |\n\n\n| # | Label Name | Label Description |\n|:--:|:--|:--| \n|1| toxic| A rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion |\n|2| severely_toxic | A very hateful, aggressive, disrespectful comment or otherwise very likely to make a user leave a discussion or give up on sharing their perspective. This attribute is much less sensitive to more mild forms of toxicity, such as comments that include positive uses of curse words. |\n|3| obscene | Swear words, curse words, or other obscene or profane language. |\n|4| threat | Describes an intention to inflict pain, injury, or violence against an individual or group.|\n|5| insult | Insulting, inflammatory, or negative comment towards a person or a group of people. |\n|6| identity_hate |\tNegative or hateful comments targeting someone because of their identity. |","metadata":{}},{"cell_type":"code","source":"train_path = \"../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\"\ntest_path = \"../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\"","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:06:07.805397Z","iopub.execute_input":"2023-11-27T05:06:07.806899Z","iopub.status.idle":"2023-11-27T05:06:07.812168Z","shell.execute_reply.started":"2023-11-27T05:06:07.806801Z","shell.execute_reply":"2023-11-27T05:06:07.810945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:06:09.297341Z","iopub.execute_input":"2023-11-27T05:06:09.297907Z","iopub.status.idle":"2023-11-27T05:06:13.656338Z","shell.execute_reply.started":"2023-11-27T05:06:09.297858Z","shell.execute_reply":"2023-11-27T05:06:13.654493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_file_size(file_path: str):\n    \"\"\"Get the size of the file in bytes\"\"\"\n    try:\n        size_bytes = os.path.getsize(file_path)\n        return size_bytes\n    except OSError: \n        print(\"No path exists at the given location\")","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:06:13.659254Z","iopub.execute_input":"2023-11-27T05:06:13.659825Z","iopub.status.idle":"2023-11-27T05:06:13.669311Z","shell.execute_reply.started":"2023-11-27T05:06:13.659774Z","shell.execute_reply":"2023-11-27T05:06:13.667759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file_memory = get_file_size(train_path)\ntest_file_memory = get_file_size(test_path)\ntotal_memory = (train_file_memory+test_file_memory)\n\n# Print the total memory usage\nprint(f'Total File Size: {total_memory / (1024 * 1024):.2f} MB')","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:06:17.820151Z","iopub.execute_input":"2023-11-27T05:06:17.820698Z","iopub.status.idle":"2023-11-27T05:06:17.829570Z","shell.execute_reply.started":"2023-11-27T05:06:17.820657Z","shell.execute_reply":"2023-11-27T05:06:17.827759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name = Section41></a>\n### **4.1 Data Description**\n\n- In this section we will get **information about the data** and see some observations.","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:06:20.129654Z","iopub.execute_input":"2023-11-27T05:06:20.130187Z","iopub.status.idle":"2023-11-27T05:06:20.230928Z","shell.execute_reply.started":"2023-11-27T05:06:20.130143Z","shell.execute_reply":"2023-11-27T05:06:20.229296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n- The **mean** of all label values seem to be significantly less than 0.5, thereby creating the case of an **imbalanced dataset** \n- All labels are **binary**, i.e., either 0 or 1","metadata":{}},{"cell_type":"markdown","source":"<a name = Section42></a>\n### **4.2 Data Information**\n\n- In this section we will see the **information about the types of features**.","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:06:23.996375Z","iopub.execute_input":"2023-11-27T05:06:23.996915Z","iopub.status.idle":"2023-11-27T05:06:24.150945Z","shell.execute_reply.started":"2023-11-27T05:06:23.996875Z","shell.execute_reply":"2023-11-27T05:06:24.149538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prelim_clean(df): \n  df = df.drop_duplicates() # Drop duplicate data, if any \n  df = df.T.drop_duplicates().T # Drop duplicate columns\n  return df \n\ndef prelim_inspection(df):\n  \"\"\"Inspects the first few rows/columns of data\"\"\" \n  display(df.head()) # look at data \n  display(df.shape)  # look a shape of data\n  display(df.iloc[:5, :5].dtypes)  # look at data types. Ideally look at all rows. Only look at first five here for minimal output\n  display(df.isna().any())\n  display(df.describe(percentiles=[0.25,0.5,0.75,0.85,0.95,0.99]))\n        \ndef null_values(df):\n  \"\"\"Checks for null values within the dataset\"\"\"\n  missing_vals = df.isnull().sum().sort_values(ascending=False)\n  missing_perc = ((df.isnull().sum())/len(df) * 100).sort_values(ascending=False)\n  return missing_vals, missing_perc ","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:06:26.205412Z","iopub.execute_input":"2023-11-27T05:06:26.205936Z","iopub.status.idle":"2023-11-27T05:06:26.217417Z","shell.execute_reply.started":"2023-11-27T05:06:26.205896Z","shell.execute_reply":"2023-11-27T05:06:26.215768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import time \nt1 = time()\ntrain_df = prelim_clean(train_df)\nprelim_inspection(train_df)\nnull_values(train_df)\nprint(f\"Total time taken for cell to run: {time()-t1}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:09:27.790958Z","iopub.execute_input":"2023-11-27T05:09:27.791929Z","iopub.status.idle":"2023-11-27T05:10:13.576552Z","shell.execute_reply.started":"2023-11-27T05:09:27.791878Z","shell.execute_reply":"2023-11-27T05:10:13.575056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t2 = time()\ntest_df = prelim_clean(test_df)\nprelim_inspection(test_df)\nnull_values(test_df)\nprint(f\"Total time taken for cell to run: {time()-t2}s\")","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:10:13.579543Z","iopub.execute_input":"2023-11-27T05:10:13.580143Z","iopub.status.idle":"2023-11-27T05:10:47.690046Z","shell.execute_reply.started":"2023-11-27T05:10:13.580098Z","shell.execute_reply":"2023-11-27T05:10:47.688787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation(s)**\n- There is only **comment_text** column as a **direct feature**. Other **discriminatory features** can    be derived from this feature\n- There exist no **null values** in **comment_text** column \n- There seem to be no **duplicate** rows in the dataset as the size remains same as earlier","metadata":{}},{"cell_type":"markdown","source":"<a name = Section5></a>\n\n---\n# **5. Data Pre-Profiling**\n---\n- For **quick analysis** pandas profiling is very handy.\n\n- Generates profile reports from a pandas DataFrame.\n\n- For each column **statistics** are presented in an interactive HTML report.","metadata":{}},{"cell_type":"code","source":"profile = ProfileReport(train_df, title=\"Profiling Report\", html={\"style\": {\"full_width\": True}})\nprofile.to_file(output_file='Pre Profiling Report.html')\nprint('Accomplished!')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T15:41:52.867367Z","iopub.execute_input":"2023-11-22T15:41:52.867736Z","iopub.status.idle":"2023-11-22T15:42:35.636089Z","shell.execute_reply.started":"2023-11-22T15:41:52.867705Z","shell.execute_reply":"2023-11-22T15:42:35.634669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation(s)** \n- There seems to be a strong relationship between \n    - toxic with **obscene** and **insult**\n    - obscene with **insult**\n    - However, the correlation values are calculated using the pearrson method, which assumes a linear relationship between continuous variables. So, to find a pattern between two categorical variables we can use other tools like\n        - Confusion Matrix/Crosstab \n        - Cramer's V statistic \n            - Cramer's V stat is an extension of the chi-square test where the extent/strength of     association is also measured \n- All labels are severely imbalanced \n        - toxic: 54.4%\n        - severe_toxic: 91.9% \n        - obscene: 70.1% \n        - threat: 97.1% \n        - insult: 71.6% \n        - identity_hate: 92.7% ","metadata":{}},{"cell_type":"markdown","source":"<a name = Section6></a>\n\n---\n# **5. Data Cleaning**\n---\n- In this section, we will perform the **cleaning** operations over the features using information from the previous section.\n\n- As a part of this project, we will employ data cleaning techniques such as working with only English text, remove special characters from the comment text, etc. *and other useful linguistic features such as **n-grams, text length, keywords, topics** etc...check this out*\n\n- To Do \n    - Add Unit Testing \n    - Assertion AND further \"is instance()\" checks \n- Order of cleaning is also important\n\n- To Remove\n    - All numbers \n    - then extra dots which occur\n    - Eg: 89.205.38.27 which follows a word \n    - Extra spaces and backslashes \n    - Extra \"\\n\" \n    - lowercase \n    - just use the standard stopwords for removal \n    - remove all numbers:\n    - Remove all nbsp \n    - Whatever I brainstorm and comes to my mind and how I have proceeded just jot it down\n  ","metadata":{}},{"cell_type":"code","source":"# English stopwords \neng_stopwords = set(stopwords.words(\"english\"))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:15:51.745325Z","iopub.execute_input":"2023-12-03T11:15:51.745774Z","iopub.status.idle":"2023-12-03T11:15:51.804330Z","shell.execute_reply.started":"2023-12-03T11:15:51.745734Z","shell.execute_reply":"2023-12-03T11:15:51.802804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load a Spacy Pipeline \nnlp = spacy.load(\"en_core_web_md\")","metadata":{"execution":{"iopub.status.busy":"2023-11-27T07:24:05.939178Z","iopub.execute_input":"2023-11-27T07:24:05.939835Z","iopub.status.idle":"2023-11-27T07:24:08.061006Z","shell.execute_reply.started":"2023-11-27T07:24:05.939778Z","shell.execute_reply":"2023-11-27T07:24:08.059963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to detect language using langdetect\n## Check if I have to perform sentence level tokenization first \n\ndef detect_language(text: str) -> str:\n    \"\"\"\n    Detect the language of the given text using langdetect library.\n\n    Parameters:\n    - text (str): The input text for language detection.\n\n    Returns:\n    - str: The detected language code (e.g., 'en' for English).\n           If language detection fails, returns 'unknown'.\n    \"\"\"\n    try:\n        # Attempt to detect the language using langdetect\n        doc = \n        return detect(text)[\"lang\"]\n    except Exception:\n        # Return 'unknown' if language detection fails\n        return \"unknown\"\n    \n# Capture the hashtags and/or usertags if any \ndef extract_hashtags(text: str) -> list:\n    \"\"\" Returns all Twitter hashtags from the text\"\"\"\n    hashtags_ls = re.findall(\"#\\w+\", text)\n    return hashtags_ls\n\n# Clean comment text \ndef clean_text(\n        text, words=True, stops=True, urls=True, tags=True, \n        newLine=True, ellipsis=True, special_chars=True, condensed=True, non_breaking_space=True, \n        character_encodings=True, stopwords=True, only_words=True) -> str:\n    \n    \"\"\" Clean tweets after extracting all hashtags and username tags\n    Not comprehensive enough to capture all idiosyncrasies, but works for most of the time\n    \"\"\"\n    \n    # Capture only words and no numbers\n    if words:\n        pattern = r\"\\d\"\n        text = re.sub(pattern, \"\", text)\n        \n    # Remove more than or equal to 2 full stops \n    if stops: \n        pattern = r\"\\.{2,}\"\n        text = re.sub(pattern, \"\", text)\n    \n    # Remove URLs \n    if urls:\n        pattern = \"(https\\:)*\\/*\\/*(www\\.)?(\\w+)(\\.\\w+)\\/*\\w*\"\n        text = re.sub(pattern, \"\", text)\n        \n    # Remove tags \n    if tags:\n        text = re.sub(\"@\\w+\", \"\", text)\n    \n    # Replacing one or more occurrences of '\\n' with ''\n    # Replacing multiple occurrences, i.e., >=2 occurrences with '.'\n    if newLine:\n        text = re.sub(\"\\n+\", \"\", text)\n        \n    # Fix contractions\n    if condensed:\n        try:\n            text = cm.fix(text)\n        except: \n            print(text)\n            \n    # Remove \"ellipsis\"\n    if ellipsis:\n        pattern = r\"\\.{2,}\"\n        text = re.sub(pattern, \"\", text)\n        \n    # Remove the special_chars list: [%, ^, *, -, _, +, =, |, \\, /, ?]\n    if special_chars:\n        spec_char_list = ['%', '^', '*', '-', '_', '+', '=', '|', '/', '?']\n        new_sent_tokens = []\n        \n        for character in text:\n            if str(character) not in spec_char_list:\n                new_sent_tokens.append(character)\n                \n        sent = \" \".join(new_sent_tokens)\n        sent = text.strip() # Add further checks for cleaning \n        \n    # Resolve character encodings\n    if character_encodings:\n        pattern = r\"â|€|¦|â|€˜|€™\"\n        text = re.sub(pattern, \"\", text)\n        \n    # Remove non-breaking space \n    if non_breaking_space: \n        pattern = r\"(\\xa0|&nbsp)\"\n        text = re.sub(pattern, \"\", text)\n        \n    # Remove stopwords\n    if stopwords:\n        words = word_tokenize(text)\n        filtered_words = [word for word in words if word not in eng_stopwords]\n        text = \" \".join(words)\n        text = text.strip()  # Add further checks for cleaning \n        \n    # Only words\n    if only_words:\n        text = re.sub(r\"[^\\w\\n\\.]+\", \" \", text)\n        text = text.strip()\n        \n    data = data.progress_apply(clean_workers)\n    \n    # Limiting length of tweet. Do this processing later. Not in first stage \n#     max_tokens = 50 \n#     min_tokens = 5 \n#     data = data.progress_apply(limit_length, min_tokens=min_tokens, max_tokens=max_tokens)\n    \n    # Dropping all NaN values, which are the token limits that didn't meet the thresholding requirements \n#     data = data.dropna()\n#     print(f\"Limited each tweet to a max. of {max_tokens} tokens and min of {min_tokens} tokens. Shape is now {data.shape}.\\n \\n. Peek: \\n {data.head()}\")\n#     visualize_lengths(data, \"Lengths of tokens after step 10\")\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2023-11-23T13:06:45.739147Z","iopub.execute_input":"2023-11-23T13:06:45.739567Z","iopub.status.idle":"2023-11-23T13:06:45.755039Z","shell.execute_reply.started":"2023-11-23T13:06:45.739537Z","shell.execute_reply":"2023-11-23T13:06:45.754039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TODO** \n- Add every preprocessing function to Spacy NLP module later in next version check\n- Convert everything to lowercase and remove multiple occurences of some repeating characters ","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_md\")\n\ndoc = nlp(u\"The shimmering azure waters lapped gently against the golden sands, painting a serene picture of tranquility under the midday sun. A gentle breeze carried the faint scent of salt and sea spray, mingling with the crisp, clean air. Seabirds soared overhead, their graceful arcs cutting through the vast expanse of the sky. Along the coastline, a cluster of cottages nestled among verdant trees, their vibrant colors standing out against the backdrop of lush foliage. Laughter and joyous chatter filled the air as families enjoyed their day by the shore, building sandcastles and playing in the surf. Farther out, sailboats dotted the horizon, their billowing sails catching the ocean's whispers, inviting adventurers to explore the endless mysteries hidden beyond the horizon.\")\n\nfor token in doc:\n    print(token.lemma_ for token in doc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_inbound = my_tokenizer(text[\"inbound\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"comment_text\"][50]","metadata":{"execution":{"iopub.status.busy":"2023-11-22T16:16:56.980322Z","iopub.execute_input":"2023-11-22T16:16:56.980740Z","iopub.status.idle":"2023-11-22T16:16:56.989455Z","shell.execute_reply.started":"2023-11-22T16:16:56.980704Z","shell.execute_reply":"2023-11-22T16:16:56.987598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_text(test_df[\"comment_text\"][50])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(detect_language(\"I am a guy.\"))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T16:17:01.251304Z","iopub.execute_input":"2023-11-22T16:17:01.251721Z","iopub.status.idle":"2023-11-22T16:17:02.307984Z","shell.execute_reply.started":"2023-11-22T16:17:01.251690Z","shell.execute_reply":"2023-11-22T16:17:02.306431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tried applying language identification earlier, however most of them were goinf unidentified \n# train_df[\"lang\"] = train_df[\"comment_text\"].apply(detect_language)\n# test_df[\"lang\"] = test_df[\"comment_text\"].apply(detect_language)\n# train_df.sample(20)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T15:47:51.299493Z","iopub.execute_input":"2023-11-15T15:47:51.299883Z","iopub.status.idle":"2023-11-15T15:47:57.998022Z","shell.execute_reply.started":"2023-11-15T15:47:51.299852Z","shell.execute_reply":"2023-11-15T15:47:57.996795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"hashtags\"] = train_df[\"comment_text\"].apply(extract_hashtags)\ntest_df[\"hashtags\"] = test_df[\"comment_text\"].apply(extract_hashtags)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T16:17:06.503722Z","iopub.execute_input":"2023-11-22T16:17:06.504158Z","iopub.status.idle":"2023-11-22T16:17:06.917229Z","shell.execute_reply.started":"2023-11-22T16:17:06.504123Z","shell.execute_reply":"2023-11-22T16:17:06.916233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"hashtags\"].sample(30)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T16:17:10.433613Z","iopub.execute_input":"2023-11-22T16:17:10.433979Z","iopub.status.idle":"2023-11-22T16:17:10.453956Z","shell.execute_reply.started":"2023-11-22T16:17:10.433949Z","shell.execute_reply":"2023-11-22T16:17:10.452355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"comment_text\"] = train_df[\"comment_text\"].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T16:17:15.968004Z","iopub.execute_input":"2023-11-22T16:17:15.968358Z","iopub.status.idle":"2023-11-22T16:19:42.809782Z","shell.execute_reply.started":"2023-11-22T16:17:15.968334Z","shell.execute_reply":"2023-11-22T16:19:42.808392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"comment_text\"] = test_df[\"comment_text\"].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T16:19:42.814767Z","iopub.execute_input":"2023-11-22T16:19:42.815185Z","iopub.status.idle":"2023-11-22T16:21:58.204375Z","shell.execute_reply.started":"2023-11-22T16:19:42.815135Z","shell.execute_reply":"2023-11-22T16:21:58.202341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Serialize train \n# Serialize test ","metadata":{"execution":{"iopub.status.busy":"2023-11-16T07:40:21.688829Z","iopub.execute_input":"2023-11-16T07:40:21.689282Z","iopub.status.idle":"2023-11-16T07:40:21.707340Z","shell.execute_reply.started":"2023-11-16T07:40:21.689250Z","shell.execute_reply":"2023-11-16T07:40:21.705852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"lang\"] = train_df[\"comment_text\"].apply(detect_language)\ntest_df[\"lang\"] = test_df[\"comment_text\"].apply(detect_language)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T16:40:28.119226Z","iopub.execute_input":"2023-11-22T16:40:28.119632Z","iopub.status.idle":"2023-11-22T16:40:51.028415Z","shell.execute_reply.started":"2023-11-22T16:40:28.119601Z","shell.execute_reply":"2023-11-22T16:40:51.027116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select only those rows where the comment language is in \"English\"\ntrain_df = train_df[train_df[\"lang\"] == \"en\"]\ntest_df = test_df[test_df[\"lang\"] == \"en\"]","metadata":{"execution":{"iopub.status.busy":"2023-11-22T16:43:18.201240Z","iopub.execute_input":"2023-11-22T16:43:18.201681Z","iopub.status.idle":"2023-11-22T16:43:18.378487Z","shell.execute_reply.started":"2023-11-22T16:43:18.201647Z","shell.execute_reply":"2023-11-22T16:43:18.376221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Again check the dataset sizes \nprint(train_df.shape)\nprint(test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T16:43:20.385771Z","iopub.execute_input":"2023-11-22T16:43:20.386238Z","iopub.status.idle":"2023-11-22T16:43:20.394681Z","shell.execute_reply.started":"2023-11-22T16:43:20.386158Z","shell.execute_reply":"2023-11-22T16:43:20.392453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Serialize the dataframes so that the entire preprocessing step need not be run again \n# Saving all my results\nwith open('train.pkl', 'wb') as handle:\n    pickle.dump(train_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open('test.pkl', 'wb') as handle:\n    pickle.dump(test_df, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T16:43:24.020896Z","iopub.execute_input":"2023-11-22T16:43:24.021397Z","iopub.status.idle":"2023-11-22T16:43:24.660781Z","shell.execute_reply.started":"2023-11-22T16:43:24.021360Z","shell.execute_reply":"2023-11-22T16:43:24.658983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name = Section6></a>\n\n---\n# **6. Exploratory Data Analysis**\n---","metadata":{}},{"cell_type":"markdown","source":"**Univariate**\n- Association between labels (Correlation) \n- Distribution of comment lengths \n- Understanding the topics behind \"toxic\" comments \n    - https://www.kaggle.com/code/jagangupta/understanding-the-topic-of-toxicity\n- Extracting **syntactic features** in text\n- Treating data imbalance \n    - Deep Learning Models \n- Metric Evaluation (Mean AUC score) \n- Clustering of topics ","metadata":{}},{"cell_type":"code","source":"# Load serialized data \ndtypes = {\n        \"toxic\": \"uint8\", \n        \"severe_toxic\": \"uint8\", \n        \"obscene\": \"uint8\", \n        \"threat\": \"uint8\", \n        \"insult\": \"uint8\", \n        \"identity_hate\": \"uint8\"\n}\nlabel_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntrain_df = pd.read_pickle(\"/kaggle/input/train-test-data/train.pkl\")\ntest_df = pd.read_pickle(\"/kaggle/input/train-test-data/test.pkl\")\ntrain_df = train_df.astype(dtypes)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:47:20.576765Z","iopub.execute_input":"2023-12-03T10:47:20.577198Z","iopub.status.idle":"2023-12-03T10:47:23.105411Z","shell.execute_reply.started":"2023-12-03T10:47:20.577165Z","shell.execute_reply":"2023-12-03T10:47:23.104262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:10:50.994059Z","iopub.execute_input":"2023-11-27T05:10:50.994481Z","iopub.status.idle":"2023-11-27T05:10:51.005163Z","shell.execute_reply.started":"2023-11-27T05:10:50.994446Z","shell.execute_reply":"2023-11-27T05:10:51.002486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:10:52.898829Z","iopub.execute_input":"2023-11-27T05:10:52.899317Z","iopub.status.idle":"2023-11-27T05:10:52.926473Z","shell.execute_reply.started":"2023-11-27T05:10:52.899282Z","shell.execute_reply":"2023-11-27T05:10:52.925163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:11:15.903733Z","iopub.execute_input":"2023-11-27T05:11:15.904230Z","iopub.status.idle":"2023-11-27T05:11:15.927042Z","shell.execute_reply.started":"2023-11-27T05:11:15.904195Z","shell.execute_reply":"2023-11-27T05:11:15.925633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations** \n- Convert the comment_text to lowercase in iteration 2 \n    - Though keep the names and self-referencing pronouns as they should be \n    - I can evern conver all the names to lower case \n        - Now check if models are sensitive to this ","metadata":{}},{"cell_type":"code","source":"# https://pandas.pydata.org/pandas-docs/stable/style.html\ndef highlight_min(data, color='coral'):\n    '''\n    highlight the maximum in a Series or DataFrame\n    '''\n    # Define the CSS attribute for background color based on the provided color\n    attr = 'background-color: {}'.format(color)\n    \n    # Check if the input data is a 1-dimensional Series (from .apply(axis=0) or axis=1)\n    if data.ndim == 1:\n        # For a Series\n        # Find positions where the value is equal to the minimum value\n        is_min = data == data.min()\n        \n        # Construct a list comprehension to apply the background color attribute\n        # to the positions of the minimum value, return an empty string otherwise\n        return [attr if v else '' for v in is_min]\n    else:\n        # For a DataFrame (from .apply(axis=None))\n        # Find positions where the value is equal to the minimum value across all columns and rows\n        is_min = data == data.min().min()\n        \n        # Create a DataFrame where cells corresponding to the minimum value\n        # receive the background color attribute, and the rest are empty strings\n        return pd.DataFrame(np.where(is_min, attr, ''),\n                            index=data.index, columns=data.columns)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:47:36.864650Z","iopub.execute_input":"2023-12-03T10:47:36.865155Z","iopub.status.idle":"2023-12-03T10:47:36.873966Z","shell.execute_reply.started":"2023-12-03T10:47:36.865117Z","shell.execute_reply":"2023-12-03T10:47:36.872651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract dataframe consisting of labels only \nlabel_df = train_df.iloc[: , 2:-2]\nlabel_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:47:40.155959Z","iopub.execute_input":"2023-12-03T10:47:40.156368Z","iopub.status.idle":"2023-12-03T10:47:40.184956Z","shell.execute_reply.started":"2023-12-03T10:47:40.156335Z","shell.execute_reply":"2023-12-03T10:47:40.183758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Question 1**: What is the relative distribution of binary values of all labels in the dataset?</h4>","metadata":{}},{"cell_type":"markdown","source":"**Note**\n- Beautify the plot later","metadata":{}},{"cell_type":"code","source":"# Convert \npos_label_df = label_df.melt(var_name=\"Label\", value_name=\"Value\")\nprint(pos_label_df.sample(20))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:47:43.358417Z","iopub.execute_input":"2023-12-03T10:47:43.358850Z","iopub.status.idle":"2023-12-03T10:47:43.425828Z","shell.execute_reply.started":"2023-12-03T10:47:43.358794Z","shell.execute_reply":"2023-12-03T10:47:43.424618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=pos_label_df, y=\"Label\", hue=\"Value\", palette=\"Set2\")\nplt.legend(loc=\"upper right\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:47:46.314238Z","iopub.execute_input":"2023-12-03T10:47:46.314670Z","iopub.status.idle":"2023-12-03T10:47:47.940090Z","shell.execute_reply.started":"2023-12-03T10:47:46.314632Z","shell.execute_reply":"2023-12-03T10:47:47.938947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Question 2:** What is the relationship between the distributions of various labels with respect to each other?</h4>","metadata":{}},{"cell_type":"markdown","source":"**Note**\n- If there exists a strong relationship between any two labels, then one of them could potentially be dropped ","metadata":{}},{"cell_type":"code","source":"print(label_df[\"toxic\"].dtype)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:47:51.381023Z","iopub.execute_input":"2023-12-03T10:47:51.381469Z","iopub.status.idle":"2023-12-03T10:47:51.388006Z","shell.execute_reply.started":"2023-12-03T10:47:51.381430Z","shell.execute_reply":"2023-12-03T10:47:51.386764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(label_df.corr())","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:47:53.029514Z","iopub.execute_input":"2023-12-03T10:47:53.030229Z","iopub.status.idle":"2023-12-03T10:47:53.067150Z","shell.execute_reply.started":"2023-12-03T10:47:53.030185Z","shell.execute_reply":"2023-12-03T10:47:53.066138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crosstab: Since a crosstab between all six classes cannot be visualized, let's take a look \n# at toxic with other tags \nmain_col=\"toxic\"\ncorr_mats=[]\nfor other_col in label_df.columns[1: ]:\n    confusion_matrix = pd.crosstab(index=label_df[main_col], columns=label_df[other_col])\n    corr_mats.append(confusion_matrix)\nout = pd.concat(corr_mats,axis=1,keys=label_df.columns[1:])\n\n#cell highlighting\nout = out.style.apply(highlight_min,axis=0)\nout","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:49:12.062611Z","iopub.execute_input":"2023-12-03T10:49:12.064020Z","iopub.status.idle":"2023-12-03T10:49:12.255668Z","shell.execute_reply.started":"2023-12-03T10:49:12.063975Z","shell.execute_reply":"2023-12-03T10:49:12.254515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**: \n- All severely toxic labels are a subset of toxic labels \n- Almost all of the rest of the sentences, which are marked **obscene**, **threat**, **insult** or **identity_hate** are also **toxic**","metadata":{}},{"cell_type":"code","source":"# Mask the upper half of the data \nmask = np.triu(np.ones_like(label_df, dtype=bool))\n\n# Create a heatmap\ncorr = label_df.corr()\nplt.figure(figsize=(10, 8))\nplt.title('Heatmap of data labels')\n\nsns.heatmap(corr, cmap=\"coolwarm\", xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot=True, fmt=\".2f\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:49:14.584512Z","iopub.execute_input":"2023-12-03T10:49:14.584955Z","iopub.status.idle":"2023-12-03T10:49:15.215319Z","shell.execute_reply.started":"2023-12-03T10:49:14.584889Z","shell.execute_reply":"2023-12-03T10:49:15.214021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n- There exists a relatively **stronger relationship** amongst **toxicity, obscenity and insult**\n- Since these are binary labels and we have utilized **Pearrson's correlation** method to compute the above values (which are more applicable for continuous-valued features), we could potentially be getting spurious results\n    - To mitigate this, we will use **Cramer V** rule for verifying the above results, which are better       suited for categorical values/labels ","metadata":{}},{"cell_type":"code","source":"def cramers_V(var1, var2):\n    \"\"\"\n    Calculate Cramér's V statistic for the association between two categorical variables.\n    \n    This function computes the Cramér's V statistic, a measure of association \n    between categorical variables. It is an extension of the chi-squared test \n    for independence and indicates the strength of association between two \n    categorical variables.\n    \n    Arguments:\n    var1 (array-like): First categorical variable.\n    var2 (array-like): Second categorical variable.\n    \n    Refer this link for further details: \n    https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n    \n    Parameters:\n    ----------\n    var1, var2 : array-like\n        Two categorical variables (e.g., arrays or Pandas Series).\n\n    Returns:\n    -------\n    float\n        Cramér's V statistic representing the strength of association \n        between var1 and var2. Values range from 0 to 1, where 0 indicates \n        no association and 1 indicates a perfect association.\n\n    \"\"\"\n    # Build a contingency table (cross-tabulation) between var1 and var2\n    crosstab = np.array(pd.crosstab(var1, var2, rownames=None, colnames=None))\n    \n    # Calculate the chi-squared statistic and retrieve the test statistic from chi2_contingency\n    stat = chi2_contingency(crosstab)[0]\n    \n    # Compute the total number of observations in the contingency table\n    obs = np.sum(crosstab)\n    \n    # Determine the minimum value between the number of rows and columns of the contingency table\n    mini = min(crosstab.shape) - 1\n    \n    # Calculate Cramér's V statistic using the formula\n    return np.sqrt(stat / (obs * mini))","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:49:17.337032Z","iopub.execute_input":"2023-12-03T10:49:17.337439Z","iopub.status.idle":"2023-12-03T10:49:17.346563Z","shell.execute_reply.started":"2023-12-03T10:49:17.337409Z","shell.execute_reply":"2023-12-03T10:49:17.345266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Calculate Kramer's Statistic \nprint(f\"Toxicity and Obscenity: {cramers_V(label_df['toxic'], label_df['obscene']):.2f}\")\nprint(f\"Toxicity and Insult: {cramers_V(label_df['toxic'], label_df['insult']):.2f}\")\nprint(f\"Obscenity and Insult: {cramers_V(label_df['obscene'], label_df['insult']):.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:49:20.716286Z","iopub.execute_input":"2023-12-03T10:49:20.716684Z","iopub.status.idle":"2023-12-03T10:49:20.783843Z","shell.execute_reply.started":"2023-12-03T10:49:20.716651Z","shell.execute_reply":"2023-12-03T10:49:20.782998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n- Thus, the correlation results returned by Pearrson's method are in fact trustworthy ","metadata":{}},{"cell_type":"markdown","source":"#### **Question 3:** What is the distribution of most common words/phrases ?</h4>","metadata":{}},{"cell_type":"code","source":"train_df[train_df.severe_toxic==1]","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:49:22.803392Z","iopub.execute_input":"2023-12-03T10:49:22.803834Z","iopub.status.idle":"2023-12-03T10:49:22.852021Z","shell.execute_reply.started":"2023-12-03T10:49:22.803785Z","shell.execute_reply":"2023-12-03T10:49:22.850242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Some Example Comments ","metadata":{}},{"cell_type":"code","source":"print(\"Severely Toxic: \")\nprint(train_df[train_df[\"severe_toxic\"]==1].iloc[3,1])\nprint(train_df[train_df[\"severe_toxic\"]==1].iloc[5,1])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:49:25.716623Z","iopub.execute_input":"2023-12-03T10:49:25.717779Z","iopub.status.idle":"2023-12-03T10:49:25.729612Z","shell.execute_reply.started":"2023-12-03T10:49:25.717728Z","shell.execute_reply":"2023-12-03T10:49:25.728270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Obscene: \")\nprint(train_df[train_df[\"obscene\"]==1].iloc[3,1])\nprint(train_df[train_df[\"obscene\"]==1].iloc[5,1])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:49:27.457254Z","iopub.execute_input":"2023-12-03T10:49:27.457717Z","iopub.status.idle":"2023-12-03T10:49:27.475751Z","shell.execute_reply.started":"2023-12-03T10:49:27.457677Z","shell.execute_reply":"2023-12-03T10:49:27.474380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Threat: \")\nprint(train_df[train_df[\"threat\"]==1].iloc[3,1])\nprint(train_df[train_df[\"threat\"]==1].iloc[5,1])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:49:29.151731Z","iopub.execute_input":"2023-12-03T10:49:29.152198Z","iopub.status.idle":"2023-12-03T10:49:29.161999Z","shell.execute_reply.started":"2023-12-03T10:49:29.152160Z","shell.execute_reply":"2023-12-03T10:49:29.160873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Insult: \")\nprint(train_df[train_df[\"insult\"]==1].iloc[3,1])\nprint(train_df[train_df[\"insult\"]==1].iloc[5,1])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:49:31.735365Z","iopub.execute_input":"2023-12-03T10:49:31.735810Z","iopub.status.idle":"2023-12-03T10:49:31.753512Z","shell.execute_reply.started":"2023-12-03T10:49:31.735773Z","shell.execute_reply":"2023-12-03T10:49:31.752347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Identity Hate: \")\nprint(train_df[train_df[\"identity_hate\"]==1].iloc[3,1])\nprint(train_df[train_df[\"identity_hate\"]==1].iloc[5,1])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T10:49:33.613285Z","iopub.execute_input":"2023-12-03T10:49:33.613708Z","iopub.status.idle":"2023-12-03T10:49:33.623977Z","shell.execute_reply.started":"2023-12-03T10:49:33.613671Z","shell.execute_reply":"2023-12-03T10:49:33.623024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations** \n- At first glance, it seems that there is hardly any difference between different types of comments\n- Note\n    - Words and letters are still in uppercase. Fix it if required \n- We would utilize wordclouds to better understand our data","metadata":{}},{"cell_type":"code","source":"!ls ../input/imagesforkernal/","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:18:01.459529Z","iopub.execute_input":"2023-12-03T11:18:01.460421Z","iopub.status.idle":"2023-12-03T11:18:02.580379Z","shell.execute_reply.started":"2023-12-03T11:18:01.460377Z","shell.execute_reply":"2023-12-03T11:18:02.579158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \" \".join(train_df[\"comment_text\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:18:04.476473Z","iopub.execute_input":"2023-12-03T11:18:04.476950Z","iopub.status.idle":"2023-12-03T11:18:04.784083Z","shell.execute_reply.started":"2023-12-03T11:18:04.476881Z","shell.execute_reply":"2023-12-03T11:18:04.782532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Toxic Comments \nsub_comment_df = train_df[(train_df[\"toxic\"]==1) | (train_df[\"severe_toxic\"]==1)][\"comment_text\"]\ntext = \"\".join(sub_comment_df)\nwc = WordCloud(background_color=\"black\", max_words=2000, stopwords=eng_stopwords)\nwc.generate(text)\nplt.figure(figsize=(9, 5))\nplt.axis(\"off\")\n# plt.title(\"Words frequented in clean toxic comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap='viridis' , random_state=17), alpha=0.98)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T13:55:57.566637Z","iopub.execute_input":"2023-11-23T13:55:57.566997Z","iopub.status.idle":"2023-11-23T13:56:00.385724Z","shell.execute_reply.started":"2023-11-23T13:55:57.566968Z","shell.execute_reply":"2023-11-23T13:56:00.384986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obscene Comments \nsub_comment_df = train_df[(train_df[\"obscene\"]==1)][\"comment_text\"]\ntext = \"\".join(sub_comment_df)\nwc = WordCloud(background_color=\"black\", max_words=2000, stopwords=eng_stopwords)\nwc.generate(text)\nplt.figure(figsize=(9, 5))\nplt.axis(\"off\")\n# plt.title(\"Words frequented in clean obscene comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap='viridis' , random_state=17), alpha=0.98)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T13:55:42.958922Z","iopub.execute_input":"2023-11-23T13:55:42.959365Z","iopub.status.idle":"2023-11-23T13:55:44.467186Z","shell.execute_reply.started":"2023-11-23T13:55:42.959292Z","shell.execute_reply":"2023-11-23T13:55:44.466406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Threat Comments \nsub_comment_df = train_df[(train_df[\"threat\"]==1)][\"comment_text\"]\ntext = \"\".join(sub_comment_df)\nwc = WordCloud(background_color=\"black\", max_words=2000, stopwords=eng_stopwords)\nwc.generate(text)\nplt.figure(figsize=(9, 5))\nplt.axis(\"off\")\n# plt.title(\"Words frequented in clean threat comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap='viridis' , random_state=17), alpha=0.98)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T13:55:50.541741Z","iopub.execute_input":"2023-11-23T13:55:50.542187Z","iopub.status.idle":"2023-11-23T13:55:51.266086Z","shell.execute_reply.started":"2023-11-23T13:55:50.542150Z","shell.execute_reply":"2023-11-23T13:55:51.265211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identity hate based comments \nsub_comment_df = train_df[(train_df[\"identity_hate\"]==1)][\"comment_text\"]\ntext = \"\".join(sub_comment_df)\nwc = WordCloud(background_color=\"black\", max_words=2000, stopwords=eng_stopwords)\nwc.generate(text)\nplt.figure(figsize=(9, 5))\nplt.axis(\"off\")\n# plt.title(\"Words frequented in clean threat comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap='viridis' , random_state=17), alpha=0.98)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:16:20.400488Z","iopub.execute_input":"2023-12-03T11:16:20.401241Z","iopub.status.idle":"2023-12-03T11:16:20.494870Z","shell.execute_reply.started":"2023-12-03T11:16:20.401203Z","shell.execute_reply":"2023-12-03T11:16:20.493334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations** \n- Enter your observations ","metadata":{}},{"cell_type":"markdown","source":"#### **Question 4:** Distribution of topics within the dataset</h4>","metadata":{}},{"cell_type":"markdown","source":"## Topic Modeling \n- Topic modeling can be a useful tool to summarize the context of a huge corpus(text) by guessing what the \"Topic\" or the general theme of the sentence.\n- This can also be used as inputs to our classifier if they can identify patterns or \"Topics\" that indicate toxicity.\n- The following steps would be involved in the process:\n    - Preprocessing \n        - Tokenize (split the documents into tokens) \n        - Lemmatize the tokens \n        - Compute bigrams \n        - Compute a bag-of-words representation of the data \n    - Lemmatization \n    - Creation of dictionary (list all words in the cleaned text) \n    - Topic Modeling using LDA \n    - Visualization with pyLDAviz\n    - Convert topics to sparse vectors \n    - Feed sparse vectors to the model \n","metadata":{}},{"cell_type":"code","source":"# Accumulate all comments in a list \ndoc_ls, processed_doc_ls = [], []\n\nfor com_text in train_df[\"comment_text\"]: \n    doc_ls.append(com_text)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:18:09.307631Z","iopub.execute_input":"2023-12-03T11:18:09.308295Z","iopub.status.idle":"2023-12-03T11:18:09.371603Z","shell.execute_reply.started":"2023-12-03T11:18:09.308259Z","shell.execute_reply":"2023-12-03T11:18:09.370294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc_ls[0: 3]","metadata":{"execution":{"iopub.status.busy":"2023-11-23T16:19:41.039712Z","iopub.execute_input":"2023-11-23T16:19:41.040809Z","iopub.status.idle":"2023-11-23T16:19:41.049302Z","shell.execute_reply.started":"2023-11-23T16:19:41.040774Z","shell.execute_reply":"2023-11-23T16:19:41.047462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2023-11-27T09:02:34.102996Z","iopub.execute_input":"2023-11-27T09:02:34.103391Z","iopub.status.idle":"2023-11-27T09:02:34.135234Z","shell.execute_reply.started":"2023-11-27T09:02:34.103363Z","shell.execute_reply":"2023-11-27T09:02:34.134033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"comment_length\"] = train_df[\"comment_text\"].apply(len)\ntest_df[\"comment_length\"] = test_df[\"comment_text\"].apply(len)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:18:11.359274Z","iopub.execute_input":"2023-12-03T11:18:11.359686Z","iopub.status.idle":"2023-12-03T11:18:11.598583Z","shell.execute_reply.started":"2023-12-03T11:18:11.359654Z","shell.execute_reply":"2023-12-03T11:18:11.597591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:36:47.634443Z","iopub.execute_input":"2023-11-27T05:36:47.635045Z","iopub.status.idle":"2023-11-27T05:38:51.388443Z","shell.execute_reply.started":"2023-11-27T05:36:47.634998Z","shell.execute_reply":"2023-11-27T05:38:51.386825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import wordnet as wn\nfrom nltk.stem.wordnet import WordNetLemmatizer\nwordnet_lemma = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:40:11.452607Z","iopub.execute_input":"2023-11-27T05:40:11.453036Z","iopub.status.idle":"2023-11-27T05:40:11.460326Z","shell.execute_reply.started":"2023-11-27T05:40:11.453003Z","shell.execute_reply":"2023-11-27T05:40:11.458675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:52:09.120515Z","iopub.execute_input":"2023-11-27T05:52:09.120982Z","iopub.status.idle":"2023-11-27T05:52:09.130862Z","shell.execute_reply.started":"2023-11-27T05:52:09.120947Z","shell.execute_reply":"2023-11-27T05:52:09.129262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatize(self, word: str, pos: str = \"n\") -> str:\n    \"\"\"Lemmatize `word` using WordNet's built-in morphy function.\n       Returns the input word unchanged if it cannot be found in WordNet.\n\n      :param word: The input word to lemmatize.\n      :type word: str\n      :param pos: The Part Of Speech tag. Valid options are `\"n\"` for nouns,\n            `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n            for satellite adjectives.\n      :param pos: str\n      :return: The lemma of `word`, for the given `pos`.\n    \"\"\"\n    lemmas = wn._morphy(word, pos)\n    return min(lemmas, key=len) if lemmas else word","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:18:18.956934Z","iopub.execute_input":"2023-12-03T11:18:18.957361Z","iopub.status.idle":"2023-12-03T11:18:18.964452Z","shell.execute_reply.started":"2023-12-03T11:18:18.957328Z","shell.execute_reply":"2023-12-03T11:18:18.963295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[lemmatize(str(word), \"n\") for word in nltk.word_tokenize(\"I am a boy with a big fat butt!!!\")]","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:54:18.867899Z","iopub.execute_input":"2023-11-27T05:54:18.868330Z","iopub.status.idle":"2023-11-27T05:54:19.141524Z","shell.execute_reply.started":"2023-11-27T05:54:18.868298Z","shell.execute_reply":"2023-11-27T05:54:19.139785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[porter_stem.stem(word) for word in nltk.word_tokenize(\"I am a boy with a big fat butt!!!\")]","metadata":{"execution":{"iopub.status.busy":"2023-11-27T06:23:17.656178Z","iopub.execute_input":"2023-11-27T06:23:17.656862Z","iopub.status.idle":"2023-11-27T06:23:17.667849Z","shell.execute_reply.started":"2023-11-27T06:23:17.656810Z","shell.execute_reply":"2023-11-27T06:23:17.666690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load the Spacy English language \n# from spacy.lang.en import English \n\n# # Load English tokenizer, tagger, Parser and NER \n# nlp = English()\n\n# # Custom pipeline components for lemmatization and adding bigrams \n# processed_docs = [], []\n\n# # Add lemmatizer to the Spacy pipeline \n# lemmatizer = nlp.add_pipe(\"lemmatizer\")\n\n# # Initialize the SpaCy pipeline to load the required data\n# nlp.initialize()\n\n\n# for doc in lemmatizer.pipe(doc_ls):\n#     print(doc)\n#     break\n    \n# Store in Processed docs \nfrom typing import List, Dict\ndef tokenizer_lemmatize(text: str) -> List[str]: \n    tokens = nltk.word_tokenize(text)\n    return [wordnet_lemma.lemmatize(token) for token in tokens]","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:36:29.912927Z","iopub.execute_input":"2023-11-27T05:36:29.913404Z","iopub.status.idle":"2023-11-27T05:36:29.920862Z","shell.execute_reply.started":"2023-11-27T05:36:29.913368Z","shell.execute_reply":"2023-11-27T05:36:29.919908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check if I have added the code for deaccentisization in the preprocessing step ","metadata":{}},{"cell_type":"code","source":"train_df[\"tokens\"] = train_df[\"comment_text\"].apply(tokenizer_lemmatize)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:36:32.088512Z","iopub.execute_input":"2023-11-27T05:36:32.088994Z","iopub.status.idle":"2023-11-27T05:36:32.470327Z","shell.execute_reply.started":"2023-11-27T05:36:32.088958Z","shell.execute_reply":"2023-11-27T05:36:32.467935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"tokens\"] = test_df[\"comment_text\"].apply(tokenizer_lemmatize) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert into lookup tables within the dictionary using doc2bow \n# print(dictionary.doc2bow(all))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ntqdm.pandas(desc=\"Processing\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:18:34.572778Z","iopub.execute_input":"2023-12-03T11:18:34.573782Z","iopub.status.idle":"2023-12-03T11:18:34.579423Z","shell.execute_reply.started":"2023-12-03T11:18:34.573741Z","shell.execute_reply":"2023-12-03T11:18:34.578160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_md\")\nprint(nlp.pipe_names)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:26:02.129555Z","iopub.execute_input":"2023-12-03T11:26:02.130334Z","iopub.status.idle":"2023-12-03T11:26:04.994736Z","shell.execute_reply.started":"2023-12-03T11:26:02.130288Z","shell.execute_reply":"2023-12-03T11:26:04.993366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_md\", exclude=[\"tok2vec\", \"parser\", \"attribute_ruler\", \"ner\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:26:09.092262Z","iopub.execute_input":"2023-12-03T11:26:09.092714Z","iopub.status.idle":"2023-12-03T11:26:11.474625Z","shell.execute_reply.started":"2023-12-03T11:26:09.092676Z","shell.execute_reply":"2023-12-03T11:26:11.473443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unidecode import unidecode\n\ndef lemmatize(text: str, min_len: int = 3) -> str:\n    doc = nlp(text)\n    \n    # Extract lemmatized text, excluding punctuation, certain parts of speech, \n    # words shorter than min_len, and de-accenting the text\n    lemmatized_text = [unidecode(token.lemma_) for token in doc if not token.is_punct and token.pos_ not in ('PRON', 'AUX', 'ADP', 'CCONJ') and len(token.lemma_) >= min_len]\n    \n    return lemmatized_text","metadata":{"execution":{"iopub.status.busy":"2023-12-03T12:56:23.072414Z","iopub.execute_input":"2023-12-03T12:56:23.072898Z","iopub.status.idle":"2023-12-03T12:56:23.080334Z","shell.execute_reply.started":"2023-12-03T12:56:23.072860Z","shell.execute_reply":"2023-12-03T12:56:23.079162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:49:20.742940Z","iopub.execute_input":"2023-12-03T11:49:20.743765Z","iopub.status.idle":"2023-12-03T11:49:20.937975Z","shell.execute_reply.started":"2023-12-03T11:49:20.743724Z","shell.execute_reply":"2023-12-03T11:49:20.936812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to seperate sentenses into words\ndef preprocess(comment):\n    \"\"\"\n    Function to build tokenized texts from input comment\n    \"\"\"\n    return gensim.utils.simple_preprocess(comment, deacc=True, min_len=3)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:49:22.727591Z","iopub.execute_input":"2023-12-03T11:49:22.728051Z","iopub.status.idle":"2023-12-03T11:49:22.733559Z","shell.execute_reply.started":"2023-12-03T11:49:22.728013Z","shell.execute_reply":"2023-12-03T11:49:22.732719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatize(\"Hey man I am really not trying to edit war . It is just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page . He seems to care more about the formatting than the actual info .\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T12:41:50.593202Z","iopub.execute_input":"2023-12-03T12:41:50.593657Z","iopub.status.idle":"2023-12-03T12:41:50.607020Z","shell.execute_reply.started":"2023-12-03T12:41:50.593621Z","shell.execute_reply":"2023-12-03T12:41:50.605657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for extracting lemma based on POS \n# def lemmatize(text: str) -> str: \n#     doc = nlp(text)\n    \n#     # Extract lemmatized text and store it back into a string\n#     # lemmatized_text = \" \".join([token.lemma_ for token in doc])\n    \n#     return [token.lemma_ for token in doc]","metadata":{"execution":{"iopub.status.busy":"2023-12-03T12:56:07.949599Z","iopub.execute_input":"2023-12-03T12:56:07.950172Z","iopub.status.idle":"2023-12-03T12:56:07.956011Z","shell.execute_reply.started":"2023-12-03T12:56:07.950125Z","shell.execute_reply":"2023-12-03T12:56:07.954899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# import multiprocessing\n\n# Using os module\ncpu_count_os = os.cpu_count()\nprint(f\"CPU count: {cpu_count_os}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:26:13.688096Z","iopub.execute_input":"2023-12-03T11:26:13.689196Z","iopub.status.idle":"2023-12-03T11:26:13.695556Z","shell.execute_reply.started":"2023-12-03T11:26:13.689146Z","shell.execute_reply":"2023-12-03T11:26:13.694384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U pandas # upgrade pandas\n!pip install swifter # first time installation\n!pip install swifter[notebook] # first time installation including dependency for rich progress bar in jupyter notebooks\n!pip install swifter[groupby] # first time installation including dependency for groupby.apply functionality","metadata":{"execution":{"iopub.status.busy":"2023-11-27T08:32:58.333814Z","iopub.execute_input":"2023-11-27T08:32:58.334278Z","iopub.status.idle":"2023-11-27T08:34:05.810460Z","shell.execute_reply.started":"2023-11-27T08:32:58.334248Z","shell.execute_reply":"2023-11-27T08:34:05.808814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"tokenized_text\"] = train_df[\"comment_text\"].progress_apply(lemmatize)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T12:56:27.790906Z","iopub.execute_input":"2023-12-03T12:56:27.791346Z","iopub.status.idle":"2023-12-03T13:00:00.962422Z","shell.execute_reply.started":"2023-12-03T12:56:27.791310Z","shell.execute_reply":"2023-12-03T13:00:00.961083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"tokenized_text\"] = test_df[\"comment_text\"].progress_apply(lemmatize)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T11:31:43.400233Z","iopub.execute_input":"2023-12-03T11:31:43.401015Z","iopub.status.idle":"2023-12-03T11:34:47.560283Z","shell.execute_reply.started":"2023-12-03T11:31:43.400966Z","shell.execute_reply":"2023-12-03T11:34:47.558924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.reset_index()\ntest_df = test_df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:03:53.434526Z","iopub.execute_input":"2023-12-03T13:03:53.434989Z","iopub.status.idle":"2023-12-03T13:03:53.578311Z","shell.execute_reply.started":"2023-12-03T13:03:53.434953Z","shell.execute_reply":"2023-12-03T13:03:53.577184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.drop(\"index\", axis=1)\ntest_df = test_df.drop(\"index\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:04:45.991108Z","iopub.execute_input":"2023-12-03T13:04:45.991630Z","iopub.status.idle":"2023-12-03T13:04:46.214904Z","shell.execute_reply.started":"2023-12-03T13:04:45.991590Z","shell.execute_reply":"2023-12-03T13:04:46.213837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:05:01.745416Z","iopub.execute_input":"2023-12-03T13:05:01.745878Z","iopub.status.idle":"2023-12-03T13:05:01.777404Z","shell.execute_reply.started":"2023-12-03T13:05:01.745841Z","shell.execute_reply":"2023-12-03T13:05:01.775986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('train_v2.pkl', 'wb') as handle:\n    pickle.dump(train_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open('test_v2.pkl', 'wb') as handle:\n    pickle.dump(test_df, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:05:13.529117Z","iopub.execute_input":"2023-12-03T13:05:13.529560Z","iopub.status.idle":"2023-12-03T13:05:23.292934Z","shell.execute_reply.started":"2023-12-03T13:05:13.529525Z","shell.execute_reply":"2023-12-03T13:05:23.291958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Serialize both the dataframes ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# corpus_token_ls = list(train_df[\"tokenized_text\"]) \n# corpus_token_ls.append(list(test_df[\"tokenized_text\"]))\nall_tokenized_text = train_df[\"tokenized_text\"].append(test_df[\"tokenized_text\"])\n# Phrases help us group together bigrams :  new + york --> new_york\n# bigram = gensim.models.Phrases(corpus_token_ls)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:21:42.610797Z","iopub.execute_input":"2023-12-03T13:21:42.611322Z","iopub.status.idle":"2023-12-03T13:21:42.948309Z","shell.execute_reply.started":"2023-12-03T13:21:42.611284Z","shell.execute_reply":"2023-12-03T13:21:42.947022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_tokenized_text = all_tokenized_text.reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:22:46.031355Z","iopub.execute_input":"2023-12-03T13:22:46.032036Z","iopub.status.idle":"2023-12-03T13:22:46.054540Z","shell.execute_reply.started":"2023-12-03T13:22:46.031972Z","shell.execute_reply":"2023-12-03T13:22:46.053149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_tokenized_text = all_tokenized_text.drop(\"index\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:23:11.251510Z","iopub.execute_input":"2023-12-03T13:23:11.252142Z","iopub.status.idle":"2023-12-03T13:23:11.280690Z","shell.execute_reply.started":"2023-12-03T13:23:11.252091Z","shell.execute_reply":"2023-12-03T13:23:11.279575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_tokenized_text","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:38:25.238614Z","iopub.execute_input":"2023-12-03T13:38:25.240036Z","iopub.status.idle":"2023-12-03T13:38:25.265476Z","shell.execute_reply.started":"2023-12-03T13:38:25.239981Z","shell.execute_reply":"2023-12-03T13:38:25.264557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Phrases help us group together bigrams :  new + york --> new_york\nbigram = gensim.models.Phrases(all_tokenized_text[\"tokenized_text\"].tolist())","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:38:47.778328Z","iopub.execute_input":"2023-12-03T13:38:47.779548Z","iopub.status.idle":"2023-12-03T13:39:39.301853Z","shell.execute_reply.started":"2023-12-03T13:38:47.779510Z","shell.execute_reply":"2023-12-03T13:39:39.300531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(all_tokenized_text)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:33:17.477298Z","iopub.execute_input":"2023-12-03T13:33:17.478264Z","iopub.status.idle":"2023-12-03T13:33:17.484983Z","shell.execute_reply.started":"2023-12-03T13:33:17.478221Z","shell.execute_reply":"2023-12-03T13:33:17.484061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bigram[all_tokenized_text[\"tokenized_text\"].iloc[32]]","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:41:38.314448Z","iopub.execute_input":"2023-12-03T13:41:38.315173Z","iopub.status.idle":"2023-12-03T13:41:38.324033Z","shell.execute_reply.started":"2023-12-03T13:41:38.315134Z","shell.execute_reply":"2023-12-03T13:41:38.323084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.corpora import Dictionary","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:46:13.773085Z","iopub.execute_input":"2023-12-03T13:46:13.773563Z","iopub.status.idle":"2023-12-03T13:46:13.779983Z","shell.execute_reply.started":"2023-12-03T13:46:13.773526Z","shell.execute_reply":"2023-12-03T13:46:13.778627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create the dictionary\ndictionary = Dictionary(all_tokenized_text[\"tokenized_text\"])\nprint(\"There are\", len(dictionary),\"number of words in the final dictionary\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:46:15.826406Z","iopub.execute_input":"2023-12-03T13:46:15.826958Z","iopub.status.idle":"2023-12-03T13:46:45.128238Z","shell.execute_reply.started":"2023-12-03T13:46:15.826913Z","shell.execute_reply":"2023-12-03T13:46:45.126941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iter(dictionary)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:49:06.048246Z","iopub.execute_input":"2023-12-03T13:49:06.048726Z","iopub.status.idle":"2023-12-03T13:49:06.064106Z","shell.execute_reply.started":"2023-12-03T13:49:06.048687Z","shell.execute_reply":"2023-12-03T13:49:06.062656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert into lookup tuples within the dictionary using doc2bow\nprint(dictionary.doc2bow((all_tokenized_text[\"tokenized_text\"].iloc[2000])))\nprint(\"Wordlist from the sentence:\", all_tokenized_text[\"tokenized_text\"].iloc[2000])\n\n#to check\nprint(\"Wordlist from the dictionary lookup:\", \n      dictionary[21], dictionary[22], dictionary[23], dictionary[24], dictionary[25], dictionary[26], dictionary[27])","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:54:28.992337Z","iopub.execute_input":"2023-12-03T13:54:28.993447Z","iopub.status.idle":"2023-12-03T13:54:29.002027Z","shell.execute_reply.started":"2023-12-03T13:54:28.993386Z","shell.execute_reply":"2023-12-03T13:54:29.000897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:55:53.988488Z","iopub.execute_input":"2023-12-03T13:55:53.989003Z","iopub.status.idle":"2023-12-03T13:55:53.994491Z","shell.execute_reply.started":"2023-12-03T13:55:53.988959Z","shell.execute_reply":"2023-12-03T13:55:53.993133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scale it to all text\nstart_time = time.time()\ncorpus = [dictionary.doc2bow(text) for text in all_tokenized_text[\"tokenized_text\"]]\nend_corpus = time.time()\nprint(\"Time till corpus creation:\", end_corpus - start_time,\"s\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:56:50.356104Z","iopub.execute_input":"2023-12-03T13:56:50.357461Z","iopub.status.idle":"2023-12-03T13:57:08.485139Z","shell.execute_reply.started":"2023-12-03T13:56:50.357412Z","shell.execute_reply":"2023-12-03T13:57:08.483898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create the LDA model\nstart_lda = time.time()\nldamodel = LdaModel(corpus=corpus, num_topics=15, id2word=dictionary)\nend_lda = time.time()\nprint(\"Time till LDA model creation:\",end_lda-start_lda,\"s\")","metadata":{"execution":{"iopub.status.busy":"2023-12-03T13:58:42.298916Z","iopub.execute_input":"2023-12-03T13:58:42.299395Z","iopub.status.idle":"2023-12-03T14:03:49.601157Z","shell.execute_reply.started":"2023-12-03T13:58:42.299357Z","shell.execute_reply":"2023-12-03T14:03:49.599935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyLDAvis","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:04:49.989959Z","iopub.execute_input":"2023-12-03T14:04:49.990393Z","iopub.status.idle":"2023-12-03T14:05:03.900494Z","shell.execute_reply.started":"2023-12-03T14:04:49.990360Z","shell.execute_reply":"2023-12-03T14:05:03.899056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyLDAvis.gensim","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:06:38.619658Z","iopub.execute_input":"2023-12-03T14:06:38.620568Z","iopub.status.idle":"2023-12-03T14:06:38.953837Z","shell.execute_reply.started":"2023-12-03T14:06:38.620500Z","shell.execute_reply":"2023-12-03T14:06:38.952534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pyLDAvis.enable_notebook()","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:06:56.374084Z","iopub.execute_input":"2023-12-03T14:06:56.374512Z","iopub.status.idle":"2023-12-03T14:06:56.380465Z","shell.execute_reply.started":"2023-12-03T14:06:56.374478Z","shell.execute_reply":"2023-12-03T14:06:56.379181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)","metadata":{"execution":{"iopub.status.busy":"2023-12-03T14:07:09.320074Z","iopub.execute_input":"2023-12-03T14:07:09.320528Z","iopub.status.idle":"2023-12-03T14:10:21.848240Z","shell.execute_reply.started":"2023-12-03T14:07:09.320495Z","shell.execute_reply":"2023-12-03T14:10:21.846533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the histogram\nplt.figure(figsize=(8, 6))\nsns.histplot(data, kde=True, bins=30, color='skyblue', edgecolor='black')\nplt.title('Distribution of Random Data')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Syntactic text features ","metadata":{"execution":{"iopub.status.busy":"2023-11-17T18:17:07.486915Z","iopub.execute_input":"2023-11-17T18:17:07.487338Z","iopub.status.idle":"2023-11-17T18:17:07.519419Z","shell.execute_reply.started":"2023-11-17T18:17:07.487303Z","shell.execute_reply":"2023-11-17T18:17:07.518506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checks \nprint(f\"Total no. of comments: {len(all_text)}\")\nprint(f\"Before preprocessing: {train.comment_text.iloc[30]}\")\nprint(f\"After preprocessing: all_text.iloc[30]\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bivariate**\n- ","metadata":{}},{"cell_type":"markdown","source":"**Multivariate**\n- ","metadata":{}},{"cell_type":"code","source":"x=train.iloc[:,2:].sum()\n#plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"# per class\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Type ', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The toxicity is not evenly spread out across classes. Hence we might face class imbalance problems\n- There are ~95k comments in the training dataset and there are ~21k tags and ~86k clean comments!?\n- This is only possible when multiple tags are associated with each comment (eg) a comment can be classified as both toxic and obscene.","metadata":{}},{"cell_type":"code","source":"# Pending Preprocessing \n    - Adding unigrams, bigrams and trigrams \n    - Sentiment of extracted hashtags: if any \n    - Check...\n# Add the code for NBSVM ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Leaky Features \n- **Caution**: Even though including these features might help us perform better in this particular scenario, it will not make sense to add him in the final model/general purpose model \n- Here we are creating our own custom **count vectorizer** to create count variables that match our regex condition \n**Note**: Use Data Version Control to capture these features from the raw data itself ","metadata":{}},{"cell_type":"code","source":"# Leaky Features \n# Extracted from the resouce: \ntrain_df[\"ip\"] = train_df[\"comment_text\"].apply(lambda text: re.findall(r\"\\d{1, 3}\\.\\d{1, 3}\\.\\d{1, 3}\\.\\d{1, 3}\", str(text))\ntest_df[\"ip\"] = test_df[\"comment_text\"].apply(lambda text: re.findall(r\"\\d{1, 3}\\.\\d{1, 3}\\.\\d{1, 3}\\.\\d{1, 3}\", str(text)))\n                                                \n# Count IPs \ntrain_df[\"count_ips\"] = train_df[\"ip\"].apply(lambda text: len(text))\ntest_df[\"count_ips\"] = test_df[\"ip\"].apply(lambda text: len(text))\n\n\n# Links \ntrain_df[\"links\"] = train_df[\"comment_text\"].apply(lambda text: re.findall(r\"(https\\:)*\\/*\\/*(www\\.)?(\\w+)(\\.\\w+)\\/*\\w*\", str(text))\ntest_df[\"links\"] = test_df[\"comment_text\"].apply(lambda text: re.findall(r\"(https\\:)*\\/*\\/*(www\\.)?(\\w+)(\\.\\w+)\\/*\\w*\", str(text)))\n                                                   \n# Count links \ntrain_df[\"count_links\"] = train_df[\"links\"].apply(lambda text: len(text)) \ntest_df[\"count_links\"] = test_df[\"links\"].apply(lambda text: len(text))\n                                                   \n                                                    \n# Article IDs...for now, I don't think this feature would be useful in any way\n\n# Username mentions\ntrain_df[\"username\"] = train_df[\"comment_text\"].apply(lambda text: re.findall(\"\\[\\[User(.*)\\|\"), str(text))\ntest_df[\"username\"] = test_df[\"comment_text\"].apply(lambda text: re.findall(\"\\[\\[User(.*)\\|\"), str(text))\n                                                                                              \n# Count Usernames \ntrain_df[\"count_usernames\"] = train_df[\"usernames\"].apply(lambda text: len(text))\ntest_df[\"count_usernames\"] = test_df[\"usernames\"].apply(lambda text: len(text))\n                                                   \n# Leaky Ip \n\n# Leaky Usernames \ncv = CountVectorizer()\ncount_feats_user = cv.fit_transform(train_df[\"usernames\"].apply(lambda text: str(text)))\n\n                                                   \ncv = CountVectorizer()\n                                                   ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking a few of the usernames ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Leaky Feature Stability \n- Checking if the features have actually overleaked\n- We might need to remove those features where the values have a lot of overlap between the training and the test set ","metadata":{}},{"cell_type":"code","source":"leaky_features = [\"ip\", \"link\", \"username\", \"count_ips\", \"count_links\", \"count_usernames\"]","metadata":{"execution":{"iopub.status.busy":"2023-12-06T11:34:14.527552Z","iopub.execute_input":"2023-12-06T11:34:14.528079Z","iopub.status.idle":"2023-12-06T11:34:14.536311Z","shell.execute_reply.started":"2023-12-06T11:34:14.528043Z","shell.execute_reply":"2023-12-06T11:34:14.534560Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}